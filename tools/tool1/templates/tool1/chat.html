<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant</title>
    <link rel="stylesheet" href="/static/style.css">
</head>

<style>
   body {
    font-family: 'Poppins', sans-serif;
    background: linear-gradient(135deg, #1e3c72, #2a5298);
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    margin: 0;
}

.container {
    text-align: center;
    background: rgba(255, 255, 255, 0.9);
    padding: 30px;
    border-radius: 15px;
    box-shadow: 0 10px 25px rgba(0, 0, 0, 0.3);
    width: 90%;
    max-width: 400px;
    animation: fadeIn 1s ease-in-out;
}

h1 {
    font-size: 2.5rem;
    color: #fff;
    text-shadow: 2px 2px 10px rgba(0, 0, 0, 0.3);
}

.mic-wrapper {
    margin-top: 20px;
    position: relative;
}

.mic-btn {
    background: radial-gradient(circle, #ff416c, #ff4b2b);
    color: white;
    font-size: 2rem;
    padding: 20px;
    border-radius: 50%;
    border: none;
    cursor: pointer;
    outline: none;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    transition: transform 0.2s ease-in-out, box-shadow 0.3s;
    animation: pulse 2s infinite;
}

.mic-btn:hover {
    transform: scale(1.1);
    box-shadow: 0 8px 20px rgba(255, 75, 43, 0.6);
}

.mic-btn:active {
    background: #c0392b;
    transform: scale(0.9);
}

@keyframes pulse {
    0% {
        transform: scale(1);
        box-shadow: 0 0 10px rgba(255, 75, 43, 0.6);
    }
    50% {
        transform: scale(1.1);
        box-shadow: 0 0 20px rgba(255, 75, 43, 0.8);
    }
    100% {
        transform: scale(1);
        box-shadow: 0 0 10px rgba(255, 75, 43, 0.6);
    }
}

#conversation-history {
    margin-top: 20px;
    font-size: 1rem;
    color: #222;
    max-height: 300px;
    overflow-y: auto;
    background: rgba(255, 255, 255, 0.8);
    padding: 10px;
    border-radius: 10px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
}

.history-entry {
    margin-bottom: 10px;
    padding: 15px;
    border: 1px solid #ccc;
    border-radius: 10px;
    background: linear-gradient(135deg, #e0eafc, #cfdef3);
    box-shadow: 0 5px 10px rgba(0, 0, 0, 0.1);
}

.user-input {
    font-weight: bold;
    color: #ff416c;
}

.ai-response {
    margin-top: 5px;
    color: #333;
}

@keyframes fadeIn {
    from {
        opacity: 0;
        transform: translateY(-20px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@media (max-width: 600px) {
    .container {
        width: 95%;
    }
    .mic-btn {
        font-size: 1.5rem;
        padding: 15px;
    }
}
</style>

<body>
    <div class="container">
        <h1>AI Voice Assistant</h1>
        <div class="mic-wrapper">
            <button id="start-record-btn" class="mic-btn">ðŸŽ¤ Speak Now</button>
        </div>
        <div id="response-text"></div>

        <div id="conversation-history">
            <!-- Conversation history will be dynamically updated here -->
        </div>
    </div>

    <!-- Include JavaScript for microphone interaction and speech synthesis -->
    <script src="/static/script.js"></script>
</body>

<script>
    const startRecordBtn = document.getElementById('start-record-btn');
    const responseText = document.getElementById('response-text');
    const conversationHistory = document.getElementById('conversation-history');

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();

    if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
        responseText.innerHTML = 'Speech Recognition API not supported in this browser.';
    }

    recognition.onstart = function() {
        responseText.innerHTML = 'Listening...';
    };

    recognition.onspeechend = function() {
        recognition.stop();
    };

    recognition.onresult = function(event) {
        const userInput = event.results[0][0].transcript;
        responseText.innerHTML = `You said: "${userInput}"`;

        // Send user input to Flask server
        fetch('/tool1/process_voice', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ user_input: userInput }),
        })
        .then(response => response.json())
        .then(data => {
            const aiResponse = data.response;
            responseText.innerHTML = `AI Response: "${aiResponse}"`;

            // Update conversation history with both user input and AI response
            updateConversationHistory(data.conversation_history);

            speakResponse(aiResponse);
        })
        .catch(error => {
            responseText.innerHTML = 'Error processing request.';
        });
    };

    startRecordBtn.addEventListener('click', () => {
        recognition.start();
    });

    // Update the conversation history section
    function updateConversationHistory(history) {
        conversationHistory.innerHTML = '';  // Clear existing history

        history.forEach(entry => {
            const historyEntry = document.createElement('div');
            historyEntry.classList.add('history-entry');

            const userInput = document.createElement('div');
            userInput.classList.add('user-input');
            userInput.innerHTML = `You: ${entry.user}`;

            const aiResponse = document.createElement('div');
            aiResponse.classList.add('ai-response');
            aiResponse.innerHTML = `AI: ${entry.ai}`;

            historyEntry.appendChild(userInput);
            historyEntry.appendChild(aiResponse);

            conversationHistory.appendChild(historyEntry);
        });
    }

    // Use the SpeechSynthesis API to speak the AI response
    function speakResponse(text) {
        const synth = window.speechSynthesis;
        const utterance = new SpeechSynthesisUtterance(text);
        synth.speak(utterance);
    }
</script>
</html>
